{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "from difflib import Differ\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def flip_sentiment(x):\n",
    "    if x == 'Negative':\n",
    "        return 'Positive'\n",
    "    elif x == 'Positive':\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def compare_sentences(sentence1, sentence2):\n",
    "    differ = Differ()\n",
    "    diff = list(differ.compare(sentence1.split(), sentence2.split()))\n",
    "\n",
    "    added_words = [word[2:] for word in diff if word.startswith('+ ')]\n",
    "    removed_words = [word[2:] for word in diff if word.startswith('- ')]\n",
    "\n",
    "    return added_words, removed_words\n",
    "\n",
    "\n",
    "\n",
    "def score_minimality(orig_sent: str, edited_sent: str, normalized: bool = True) -> float:\n",
    "        \"\"\"\n",
    "          Calculate Levenshtein distance(token-level) indicating the minimality of changes between two sentences.\n",
    "          This method takes in an original sentence and an edited sentence, both as strings.\n",
    "          It calculates the Levenshtein edit distance between the tokenized versions of these sentences,\n",
    "          representing the minimum number of single-token edits needed to transform one into the other.\n",
    "          Parameters:\n",
    "          - orig_sent (str): The original sentence before editing.\n",
    "          - edited_sent (str): The edited version of the sentence.\n",
    "          - normalized (bool, optional): If True, returns a normalized score relative to the length of\n",
    "            the original sentence. If False, returns the raw edit distance value.\n",
    "          Returns:\n",
    "          - float: The calculated minimality score. If ‘normalized’ is True, the score represents the\n",
    "            proportion of changes relative to the original sentence length.u\n",
    "            Source:\n",
    "          \"\"\"\n",
    "        nlp = English()\n",
    "        tokenizer = nlp.tokenizer\n",
    "        tokenized_original = [t.text for t in tokenizer(orig_sent)]\n",
    "        tokenized_edited = [t.text for t in tokenizer(edited_sent)]\n",
    "        levenshtein_dist = nltk.edit_distance(tokenized_original, tokenized_edited)\n",
    "        if normalized:\n",
    "            return levenshtein_dist / len(tokenized_original)\n",
    "        else:\n",
    "            return levenshtein_dist\n",
    "\n",
    "\n",
    "def compute_dist(s1, s2):\n",
    "    #assert((df[SENT_COLUMN] != df[CF_SENT_COLUMN]).all())\n",
    "    assert len(s1) == len(s2)\n",
    "    dist = []\n",
    "\n",
    "    for x, y in zip(s1,s2):\n",
    "            dist.append(score_minimality(x, y))\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['test']\n",
    "TASK = 'sentiment'\n",
    "TASK_expert = 'IMDb'\n",
    "#LLM = 'llama2-20231209'\n",
    "split_name = SPLITS[0]\n",
    "\n",
    "LLMS = ['gpt3.5-20240313', 'gpt4-20240318', 'llama2_70b-20240318', 'llama2-20231209', 'mistral_56b-20240320', 'mistral-20240118']\n",
    "#LLMS = LLMS[]\n",
    "#LST = [LLM.split('-')[0] if x=='LLM' else x for x in LST]\n",
    "for LLM in LLMS:\n",
    "\n",
    "    df = pd.read_csv('../counterfactually-augmented-data/{}/combined/paired/{}_paired.tsv'.format(TASK, split_name), sep='\\t')\n",
    "\n",
    "    df_crowd = df.iloc[::2].reset_index(drop=True)  # Select rows with even indices\n",
    "    df_crowd_cfs = df.iloc[1::2].reset_index(drop=True)  # Select rows with odd indice\n",
    "\n",
    "\n",
    "\n",
    "    df_expert = pd.read_csv('../contrast-sets/{}/data/{}_original.tsv'.format(TASK_expert, split_name), sep='\\t')\n",
    "    df_expert_cfs = pd.read_csv('../contrast-sets/{}/data/{}_contrast.tsv'.format(TASK_expert, split_name), sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "    df_llm = pd.read_csv('../llms-ppl-preds/{}/{}/orig/{}.tsv'.format(LLM, TASK, split_name), \n",
    "    sep='\\t')\n",
    "\n",
    "    cfs_path = '../llms-ppl-preds/{}/{}/new/{}.tsv'.format(LLM, TASK, split_name)\n",
    "    df_llm_cfs = pd.read_csv(cfs_path, sep='\\t')\n",
    "\n",
    "    assert(len(df_llm) == len(df_llm_cfs))\n",
    "    print(LLM)\n",
    "    print(df_llm.iloc[-1]['Text'])\n",
    "    print(df_llm_cfs.iloc[-1]['Text'])\n",
    "    print('-'*20)\n",
    "    \n",
    "    df_llm_cfs['dist'] = compute_dist(df_llm_cfs['Text'], df_llm['Text'])\n",
    "\n",
    "    df_llm_cfs.to_csv(cfs_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../counterfactually-augmented-data/{}/combined/paired/{}_paired.tsv'.format(TASK, split_name), sep='\\t')\n",
    "\n",
    "df_crowd = df.iloc[::2].reset_index(drop=True)  # Select rows with even indices\n",
    "df_crowd_cfs = df.iloc[1::2].reset_index(drop=True)  # Select rows with odd indice\n",
    "\n",
    "\n",
    "\n",
    "preds_path = '../llms-ppl-preds/counterfactually-augmented-data/{}/new/{}.tsv'.format(TASK, split_name)\n",
    "df_preds = pd.read_csv(preds_path, sep='\\t')\n",
    "\n",
    "assert(len(df_crowd_cfs) == len(df_preds))\n",
    "\n",
    "df_preds['dist'] = compute_dist(df_crowd['Text'], df_crowd_cfs['Text'])\n",
    "df_preds.to_csv(preds_path, sep= '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert = pd.read_csv('../contrast-sets/{}/data/{}_original.tsv'.format('IMDb', split_name), sep='\\t')\n",
    "df_expert_cfs = pd.read_csv('../contrast-sets/{}/data/{}_contrast.tsv'.format('IMDb', split_name), sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "preds_path = '../llms-ppl-preds/contrast-sets/IMDb/data/{}.tsv'.format(split_name)\n",
    "df_preds_expert = pd.read_csv(preds_path, sep='\\t')\n",
    "\n",
    "assert(len(df_expert) == len(df_preds_expert))\n",
    "\n",
    "df_preds_expert['dist'] = compute_dist(df_crowd['Text'], df_crowd_cfs['Text'])\n",
    "df_preds_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_expert.to_csv(preds_path, sep= '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
