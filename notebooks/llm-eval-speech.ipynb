{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "result = list(Path(\"../llm-eval-gpt4/speech\").rglob(\"*.csv\"))\n",
    "result = [str(x) for x in result]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(files): \n",
    "    metrics = ['flip_label', 'minimal_change', 'realistic']#, 'grammar', 'cohesive', 'likability']\n",
    "\n",
    "    modes = ['mean']#, 'max', 'min']\n",
    "    dict = {\n",
    "        'LLM' : [],\n",
    "        'avg' : [],\n",
    "    }\n",
    "\n",
    "    for m in metrics:\n",
    "        for mode in modes:\n",
    "            dict[m + ' - ' + mode] = []\n",
    "\n",
    "\n",
    "    for f in files:\n",
    "        llm_name = f.split('/')[-2]\n",
    "\n",
    "        if 'wrong' in llm_name:\n",
    "            continue \n",
    "        df = pd.read_csv(f)\n",
    "        dict['LLM'].append(llm_name)\n",
    "        for m in metrics:\n",
    "            for mode in modes:\n",
    "                if mode == 'mean':\n",
    "                    dict[m + ' - ' + mode].append(df[m].mean().round(2))\n",
    "                if mode == 'max':\n",
    "                    dict[m + ' - ' + mode].append(df[m].max().round(2))\n",
    "                if mode == 'min':\n",
    "                    dict[m + ' - ' + mode].append(df[m].min().round(2))\n",
    "        for mode in ['mean']:\n",
    "            dict['avg'].append(np.mean([dict[m + ' - ' + mode][-1] for m in metrics if  'minimal_change_2' not in m]))\n",
    "\n",
    "    df_results = pd.DataFrame.from_dict(dict)\n",
    "    df_results['avg'] = df_results['avg'].round(2)\n",
    "    return df_results.sort_values(by=['LLM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = get_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_results.to_csv('./tables/eval_sent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = mean_results[['LLM', 'flip_label - mean', 'minimal_change - mean', 'realistic - mean', 'avg']]\n",
    "print(table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3.66+2.95+2.58)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(files, attribute = 'flip_label'): \n",
    "    metrics = [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "    dict = {\n",
    "        'LLM' : []\n",
    "    }\n",
    "\n",
    "    for m in metrics:\n",
    "        dict[str(m)] = []\n",
    "\n",
    "\n",
    "    for f in files:\n",
    "        llm_name = f.split('/')[-2]\n",
    "        dict['LLM'].append(llm_name)\n",
    "\n",
    "        df = pd.read_csv(f)\n",
    "        counts = df[attribute].value_counts(normalize=True)\n",
    "\n",
    "        for m in metrics:\n",
    "            try:\n",
    "                dict[str(m)].append(counts[m]*100)\n",
    "            except KeyError:\n",
    "                dict[str(m)].append(0)\n",
    "    \n",
    "\n",
    "    df_results = pd.DataFrame.from_dict(dict)\n",
    "    for m in metrics:\n",
    "        df_results[str(m)] = df_results[str(m)].round(2)\n",
    "\n",
    "    return df_results.sort_values(by=['LLM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(result, attribute='flip_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(result, attribute='minimal_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution(result, attribute='realistic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
